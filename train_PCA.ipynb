{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.decomposition import RandomizedPCA as PCA\n",
    "from time import time\n",
    "import re\n",
    "\n",
    "# for plotting in jupyter\n",
    "%matplotlib inline \n",
    "\n",
    "# constants\n",
    "TRAIN_DIR = './imgs/train/'\n",
    "STD_SIZE = (100, 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Training Image Paths and Split into Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_imgs = []\n",
    "for subdir in os.listdir(TRAIN_DIR):\n",
    "    path = TRAIN_DIR + subdir + '/'\n",
    "    all_imgs += [path + f for f in os.listdir(path)]\n",
    "    \n",
    "all_imgs = np.array(all_imgs)\n",
    "n_samples = len(all_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22424 Images split into 16738 training and 5686 validation images.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1123)\n",
    "val_pct = 1./4\n",
    "in_train = np.random.uniform(size = n_samples) > val_pct\n",
    "train_imgs, val_imgs = all_imgs[in_train], all_imgs[~in_train]\n",
    "print '{} Images split into {} training and {} validation images.'.format(n_samples, len(train_imgs), len(val_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the training images for input into PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_images(file_ary, resize):\n",
    "    \"\"\"\n",
    "    Given a list of image files, resize and convert to flattened array of grayscale pixel intensities.\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_ary : ndarray\n",
    "        file paths to be processed\n",
    "    resize : tuple of ints\n",
    "        width, height of new image\n",
    "    Return\n",
    "    ------\n",
    "    numpy array (n_sample, w*h) of grayscale pixel intensities.\n",
    "    \"\"\"\n",
    "    n = len(file_ary)\n",
    "    pixel_matrix = np.zeros((n, np.prod(resize)))\n",
    "    t0 = time()\n",
    "    for i, fpath in enumerate(file_ary):\n",
    "        img = cv2.imread(fpath, 0) # 0 flag converts to grayscale during load\n",
    "        resized = cv2.resize(img, resize)\n",
    "        pixel_matrix[i] = resized.reshape(-1)\n",
    "    \n",
    "    print 'Processed {0} images in {1:.{2}f} seconds'.format(n, time() - t0, 3) \n",
    "        \n",
    "    return pixel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16738 images in 63.720 seconds\n"
     ]
    }
   ],
   "source": [
    "train_proc = preprocess_images(file_ary=train_imgs, resize=STD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 principal components fit in 10.685 seconds\n"
     ]
    }
   ],
   "source": [
    "N_COMPONENTS = 200\n",
    "SEED = 2718\n",
    "pca = PCA(n_components=N_COMPONENTS, whiten=True, random_state=SEED)\n",
    "\n",
    "t0 = time()\n",
    "pca.fit(train_proc)\n",
    "print '{0} principal components fit in {1:.{2}f} seconds'.format(N_COMPONENTS, time() - t0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Images for Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_labels(file_ary):\n",
    "    \"\"\"\n",
    "    Get the class labels for a list of images, given their path.\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_ary : ndarray\n",
    "        Image file paths\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary with two entries: img, y\n",
    "    Each entry contains an aligned sequence of img file names and their class label\n",
    "    \"\"\"\n",
    "    d = {'img' : [], 'y' : []}\n",
    "    for fname in file_ary:\n",
    "        img_id = re.search('img_.*', fname).group(0)\n",
    "        label = re.search('c[0123456789]', fname).group(0)\n",
    "        d['img'].append(img_id)\n",
    "        d['y'].append(label)\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = get_labels(train_imgs)\n",
    "trainX = pca.transform(train_proc)\n",
    "trainY = train_labels['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5686 images in 22.885 seconds\n"
     ]
    }
   ],
   "source": [
    "val_proc = preprocess_images(file_ary=val_imgs, resize=STD_SIZE)\n",
    "val_labels = get_labels(val_imgs)\n",
    "valX = pca.transform(val_proc)\n",
    "valY = val_labels['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 79726 images in 284.428 seconds\n"
     ]
    }
   ],
   "source": [
    "TEST_DIR = './imgs/test/'\n",
    "test_files = os.listdir(TEST_DIR)\n",
    "test_paths = [TEST_DIR + f for f in test_files]\n",
    "test_proc = preprocess_images(file_ary=test_paths, resize=STD_SIZE)\n",
    "testX = pca.transform(test_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a random forest classifier and a support vector machine, and use a validation set to compare the two. The best model will be retrained with the full training set before making predictions on the test data for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   44.6s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 87.797 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "rf = RF(n_estimators = 800, n_jobs=-1, verbose=1, random_state=SEED)\n",
    "rf.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_error(model):\n",
    "    pred_tr = model.predict_proba(trainX)\n",
    "    pred_val = model.predict_proba(valX)\n",
    "    loss_tr = log_loss(y_true = trainY, y_pred = pred_tr)\n",
    "    loss_val = log_loss(y_true = valY, y_pred = pred_val)\n",
    "    print 'Training Log Loss : {}, Validation Log Loss : {}'.format(loss_tr, loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_error(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model is overfitting a little bit, but 0.377 on the validation set is a great start. The variance can be reduced by squashing the max tree depth, which may be worth investigating in the future. First, see if SVM gives similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf', probability=True, cache_size=3000, class_weight='balanced', random_state=SEED) \n",
    "param_grid = {'C' : np.logspace(-3, 8, num=3), 'gamma' : np.logspace(-6, -3, num=3)}\n",
    "clf = GridSearchCV(estimator=svm, param_grid=param_grid, scoring='log_loss', n_jobs=-1, cv=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 79.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 5474.668 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "clf.fit(trainX, trainY)\n",
    "print 'Training time: {0:.3f} seconds'.format(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=316.2277660168379, cache_size=3000, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=2718, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Log Loss : 0.00290193608158, Validation Log Loss : 0.0295990117968\n"
     ]
    }
   ],
   "source": [
    "get_error(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(clf, filename):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : object\n",
    "        must have a predict_proba method\n",
    "    filename : str\n",
    "        CSV filename for output\n",
    "    \"\"\"\n",
    "    predictions = clf.predict_proba(testX)\n",
    "    df1 = pd.DataFrame({'img' : test_files})\n",
    "    if hasattr(clf, 'best_estimator_'):\n",
    "        col_names = clf.best_estimator_.classes_\n",
    "    else:\n",
    "        col_names = clf.classes_\n",
    "    df2 = pd.DataFrame(predictions, columns = col_names)\n",
    "    pd.concat([df1, df2], axis=1).to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    5.3s finished\n"
     ]
    }
   ],
   "source": [
    "make_predictions(rf, 'testY_8pm_519.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_predictions(clf, 'testY_10am_520.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
